{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from transformers import get_scheduler\n",
    "from eval_metrics import print_metrics_binary\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# classifier = nn.Sequential(\n",
    "#     nn.Linear(in_features=768, out_features=2, bias=True),\n",
    "#     nn.Softmax(dim=1)\n",
    "# )\n",
    "# model = nn.Sequential(model, classifier)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4749, -0.2102, -0.2954,  ...,  0.0859,  0.3189, -0.4628],\n",
       "         [ 0.4604,  0.1088,  0.2628,  ...,  0.6170,  0.0967, -0.2721],\n",
       "         [ 0.3512, -0.1699,  0.0030,  ...,  0.2668, -0.0499, -0.3658],\n",
       "         ...,\n",
       "         [-0.2661, -0.6352, -0.0016,  ...,  0.3084,  0.6158, -0.2966],\n",
       "         [ 0.1039, -0.2937, -0.1192,  ...,  0.4055,  0.1102, -0.4169],\n",
       "         [ 0.8741,  0.8321, -0.3052,  ..., -0.8622,  1.2682, -0.1773]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.0930, -0.1334,  0.9718, -1.0000,  0.9996,  0.9787, -0.0738, -0.6276,\n",
       "         -0.0529,  0.1257,  0.9969,  0.9999, -0.9970, -0.7683, -0.1719, -0.5314,\n",
       "          1.0000,  0.1479, -0.9952, -0.2338, -0.1088, -0.9090, -0.0530,  0.9998,\n",
       "          0.1128,  0.0490,  0.9999,  0.9960, -0.3077, -0.0030, -0.1559, -0.9990,\n",
       "          0.9995, -0.9990, -0.0295,  0.0740, -0.2439,  0.0292,  0.9845, -0.9993,\n",
       "          0.0887, -0.9412,  0.2927,  0.1839,  0.9998, -0.1728, -0.1888,  0.0764,\n",
       "          0.0600,  0.4889,  0.2495,  0.9756, -0.9999,  0.9972,  1.0000, -0.0844,\n",
       "          1.0000, -0.2068, -0.9439, -0.0289, -0.0366,  0.0980, -0.5198,  0.0334,\n",
       "          0.1374,  0.1462, -0.9939, -0.1029,  0.0861, -0.2682, -0.2409,  0.0212,\n",
       "          0.9528,  0.3864, -0.0288,  0.1695, -0.1390, -0.8931,  0.9971,  0.9989,\n",
       "          0.8837, -0.9998,  0.9996, -0.1256, -0.1383,  0.0456, -0.7931, -0.9999,\n",
       "         -0.1700,  0.1869,  0.9999, -0.9982,  0.0195, -0.9988,  0.9972,  0.3581,\n",
       "          0.1241, -0.0552,  0.9999, -0.2710,  0.2726,  0.9999,  0.9109,  0.3461,\n",
       "          0.9940,  0.1557,  0.0659, -0.9906,  0.3462, -0.0212,  0.2667,  0.1959,\n",
       "         -0.2547, -0.0764,  0.9996, -0.9616,  1.0000,  0.8859,  0.0670,  0.9963,\n",
       "          0.0958,  0.9992,  1.0000,  0.8668,  0.5377,  0.1061, -0.1682,  0.9994,\n",
       "         -0.0173, -0.0072, -0.0517, -1.0000, -0.9976,  1.0000,  0.1018,  0.9996,\n",
       "         -1.0000,  0.9996, -0.9683, -0.6743, -0.1118,  0.0877, -0.9999,  0.0440,\n",
       "          0.6364, -0.1862, -0.9998, -0.9320,  0.2257, -0.7882,  0.1629, -0.1513,\n",
       "          0.1021,  0.7505,  0.9999,  0.9998,  0.9999, -0.1394, -0.7702,  0.9950,\n",
       "          0.9980, -1.0000,  0.4333, -0.9985,  0.8530,  0.9568,  0.1448, -0.3503,\n",
       "          1.0000, -0.1248,  0.0627,  0.1244,  0.0205, -0.9632, -0.1273, -0.1203,\n",
       "         -0.0975,  0.9791, -0.9999,  0.6388, -0.6254,  0.2646,  0.0165,  0.9857,\n",
       "         -1.0000, -0.7477, -0.9983, -0.0880,  0.2233,  0.1924,  0.1099, -0.1529,\n",
       "          0.8176, -0.1030,  0.0869,  0.1149,  0.9584, -0.0096,  0.9993, -0.1256,\n",
       "         -1.0000, -0.5731,  0.9998, -0.0864, -0.0353, -0.1272,  0.1473,  0.0563,\n",
       "          0.7840, -0.9008, -0.0970, -0.0106,  0.0339, -0.0568,  0.9914, -0.3397,\n",
       "         -0.1086, -0.0669, -0.2227,  0.9995, -1.0000,  0.2049, -0.2578, -1.0000,\n",
       "         -1.0000,  0.9854,  0.0515, -0.5828,  0.0813,  0.2674,  0.0021,  1.0000,\n",
       "          0.9989, -0.0637, -0.1847, -0.9968, -0.5872,  0.2536, -0.9998, -0.0348,\n",
       "         -0.0690, -0.0946,  0.4385, -0.9957,  0.0060,  0.0521, -0.9996,  0.1437,\n",
       "         -0.0626, -0.9874,  0.1469, -0.9337, -0.9256,  1.0000, -0.9982,  0.9990,\n",
       "          0.8877, -1.0000,  0.5945, -0.9741, -0.0664, -0.9783,  0.0766,  0.1790,\n",
       "          0.0400,  0.1493,  1.0000, -0.0828, -0.7025,  0.1600, -0.9990, -0.1399,\n",
       "          0.0023,  1.0000,  0.7272, -0.5162,  0.6069,  0.8970, -0.9991, -0.9989,\n",
       "          0.9966,  0.3725, -1.0000,  0.0537,  0.9998, -0.7270,  0.2370, -0.9878,\n",
       "         -0.9702, -1.0000,  0.0245, -0.1648,  0.0825,  0.9148, -0.0057,  0.0867,\n",
       "          0.9993,  0.9741,  0.1200, -0.0799, -0.0083, -0.9976, -0.1646,  0.0066,\n",
       "         -0.2195, -0.9838,  0.9999, -0.9998,  0.9925,  0.9993, -0.5949, -0.1427,\n",
       "          0.0651, -0.9998, -0.0923,  0.9959,  0.9054,  0.0381, -0.0589,  0.9965,\n",
       "         -0.0486,  0.0391,  0.1854,  0.0456, -0.0392, -0.0078,  0.9996, -0.1138,\n",
       "         -0.9999,  0.9990, -0.0148, -0.1689, -0.9760, -0.1001,  1.0000,  0.1332,\n",
       "          0.0309,  0.0848,  0.7616, -0.6746, -0.0361, -0.9999, -0.0689,  0.9663,\n",
       "          0.9903, -0.9987,  0.9926,  0.0083,  0.9922, -0.8881,  0.9952, -0.9982,\n",
       "         -0.0832,  0.1716, -0.9978, -0.0526,  0.9951,  0.9921,  0.9935,  0.3978,\n",
       "         -0.0637, -0.2584, -0.2781, -0.9998,  0.0425, -0.5617, -0.1152,  0.9540,\n",
       "          0.9311,  0.1639,  0.0680, -0.9999,  0.0248, -0.8658, -0.9866,  0.0166,\n",
       "         -0.9840,  0.2335,  0.6973, -0.2115,  0.9997, -0.0297, -0.9443,  0.9840,\n",
       "          0.5974,  1.0000, -0.9998,  0.3132,  0.9953,  0.1064, -0.8475, -0.2255,\n",
       "          0.6194, -0.7528, -0.0572, -0.9999, -0.0271,  0.1821, -0.1783, -0.0279,\n",
       "         -0.0067,  0.1081,  0.9999, -0.0114, -0.1212, -0.1831,  0.9791,  0.0122,\n",
       "         -0.0017,  0.0891, -0.1509, -0.0073,  0.1038,  0.9247,  0.2457,  0.9485,\n",
       "          0.1587, -0.9997, -0.8295, -0.9751, -0.9128, -0.0061, -0.9955,  1.0000,\n",
       "          0.9860, -0.9958, -0.9623, -0.9950, -0.9764,  0.9934, -0.0049,  0.1239,\n",
       "         -0.0047, -0.6612,  0.0489,  0.0100,  0.0183,  0.0287, -0.0503, -0.9550,\n",
       "          0.1107, -0.9985, -0.3780,  0.9977, -0.9925, -0.9673, -0.9776,  0.8366,\n",
       "          0.2655, -0.1642,  0.9199, -0.1602, -0.0097, -0.6854,  0.7682, -0.8117,\n",
       "         -0.1091, -0.3293,  0.0703,  0.9734, -0.0333, -0.0978, -0.0226, -0.9552,\n",
       "          0.9999, -0.9999, -0.7053, -0.9913, -0.1555, -0.6671, -0.9919, -0.0809,\n",
       "          0.9842,  0.9169,  0.9937, -0.0018,  0.0144,  0.0409, -0.0242, -0.9652,\n",
       "          0.7443,  0.0761,  0.2095, -0.2171,  0.9915,  0.9118, -0.9887, -0.9999,\n",
       "          0.9987, -0.1356, -0.0479,  0.0560,  0.2372, -0.1129,  0.1178, -1.0000,\n",
       "         -0.9940,  1.0000, -0.0477,  0.9458,  0.9974,  0.9967, -0.2335,  0.0892,\n",
       "         -0.9997, -0.7815,  0.1925,  0.2286, -0.9998,  0.9191, -0.9917,  0.0396,\n",
       "         -0.1047,  0.9997,  0.9999, -0.1352, -0.9998, -0.9946, -0.9509,  0.1353,\n",
       "          0.9674,  0.1465,  0.0966, -0.3009, -0.3311,  0.9894, -0.8124, -0.0352,\n",
       "         -0.9935,  1.0000, -0.2458, -0.9999,  0.9987, -0.9926,  0.7853,  0.0431,\n",
       "          0.9586, -0.2692, -0.7544,  0.9974, -0.9853,  0.9762, -0.9965, -0.7392,\n",
       "          0.9999, -1.0000, -0.1863, -1.0000, -0.9999,  0.0814,  0.0247,  0.0696,\n",
       "          0.9955, -1.0000, -0.9997, -0.0431, -0.9997, -0.8758,  0.9998, -0.1006,\n",
       "          0.9264, -0.2648, -0.1321,  0.1810,  0.9872,  0.9952,  0.2689,  0.4846,\n",
       "         -0.9999,  0.9963, -0.1932,  0.0875,  0.9999,  0.0789, -0.1691, -0.0405,\n",
       "         -1.0000, -0.0125,  0.0620, -0.5970,  0.9979, -0.0703, -0.0081, -0.0996,\n",
       "         -0.0762, -0.9972, -0.1718, -0.9934,  0.9997, -0.9997, -0.0864, -0.0992,\n",
       "          0.0425,  0.0640,  0.9980,  1.0000, -0.9236, -0.0738,  0.9996, -0.1911,\n",
       "          0.9685, -1.0000,  0.0091,  0.8627,  0.2002,  0.9997, -0.0381,  0.0254,\n",
       "          0.9479, -1.0000, -0.9997,  0.1788, -0.1007, -0.0155, -0.9999,  0.1680,\n",
       "          0.9949, -0.1352, -1.0000,  0.2451, -1.0000,  0.1190,  0.9826,  0.5246,\n",
       "          0.9999, -0.0250,  0.0714,  0.1695, -1.0000, -0.9973, -0.0016,  0.0693,\n",
       "         -0.9925,  0.9344, -0.0891,  0.7337, -0.9149, -0.0171,  0.9961, -0.0552,\n",
       "         -0.3720, -0.0831,  0.0265, -0.9994,  0.0933, -0.0171,  0.9572, -0.9961,\n",
       "         -0.3026, -0.3117,  0.9914, -1.0000, -0.0925, -0.9994,  0.0089, -0.1750,\n",
       "         -0.0493, -0.1640,  0.0605,  0.7809, -0.9997,  1.0000, -1.0000, -0.9999,\n",
       "          1.0000, -0.1258, -1.0000,  0.3149,  0.0361,  0.3762, -0.0053, -0.1673,\n",
       "          0.0711,  0.1250, -0.9873,  0.8743, -0.8421, -0.8992,  0.0802,  0.2058,\n",
       "         -0.9856,  1.0000,  0.9999,  0.9998, -0.9999, -0.0166,  0.0225,  1.0000,\n",
       "          0.0523,  0.1291,  0.9998,  0.9865,  0.1656,  0.9431,  0.0246,  0.0361,\n",
       "          0.1143,  0.1576,  0.6102, -0.9998, -0.0613, -0.9986, -0.9784,  0.9673,\n",
       "         -0.0491,  0.9993, -0.0097,  0.2749, -0.0401,  0.9999, -0.9998, -0.0674,\n",
       "         -0.9938, -0.0560, -0.9967, -0.9989, -0.2331, -0.2524, -0.9969, -0.9413,\n",
       "         -0.2006, -0.9962,  0.9949, -0.9999, -0.9995, -0.9907,  0.6783,  0.0458,\n",
       "         -0.3225,  0.9841, -0.6769,  1.0000,  0.0246, -0.3029, -0.0124, -0.1023,\n",
       "         -0.0036, -0.9991, -0.9716, -0.9999,  0.9127, -0.9996,  0.3777,  0.9999,\n",
       "          1.0000, -0.9693, -1.0000,  0.9989,  0.0320,  1.0000, -0.0301, -0.9988,\n",
       "         -0.9996, -0.0673,  0.0585,  0.9999,  0.1090,  0.2669, -0.1518, -0.1485,\n",
       "          0.2986,  0.0412, -0.1796,  0.0473,  0.0918,  0.9957, -0.3197,  1.0000]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"you are a fool.\"\n",
    "tokenized = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# print(tokenized)\n",
    "outputs = model(tokenized[\"input_ids\"])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)\n",
    "print(outputs.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4887, 0.5113]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=768, out_features=2, bias=True),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "pred = Classifier(outputs.pooler_output)\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
